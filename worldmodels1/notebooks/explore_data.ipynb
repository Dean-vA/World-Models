{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"./../data/collected_data.npy\", allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first episode's data\n",
    "episode_data = data[0]\n",
    "\n",
    "# Extract the states (images)\n",
    "states = [x[0] for x in episode_data]\n",
    "\n",
    "# Take some samples (e.g., first 5)\n",
    "sample_states = states[0:999]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = 40\n",
    "for idx, image in enumerate(sample_states):\n",
    "    # plot with 10 columns and number of rows based on number of samples\n",
    "    plt.subplot(int(np.ceil(len(sample_states)/cols)), cols, idx+1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    # add some vertical space between the images\n",
    "    plt.subplots_adjust(hspace=0.6)\n",
    "    # label the axes with the action\n",
    "    plt.title(np.round(episode_data[idx][1],2), fontsize=3)\n",
    "    #make the length of the figure relative to the number of samples\n",
    "    plt.gcf().set_size_inches(20, len(sample_states)/cols)\n",
    "    \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flattened_data = []\n",
    "episode_count = 1\n",
    "\n",
    "for episode in data:\n",
    "    for step in episode:\n",
    "        # Convert step to list and prepend episode_count\n",
    "        step_list = [episode_count] + list(step)\n",
    "        flattened_data.append(step_list)\n",
    "    episode_count += 1\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    flattened_data,\n",
    "    columns=[\"Episode\", \"State\", \"Action\", \"Reward\", \"Done\", \"Truncated\"]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(df['Reward'].values, bins=100, kde=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aswegen.d\\Dropbox\\0_Buas\\BuasDev\\World Models\\worldmodels1\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from worldmodels1.src.worldmodels1.cnnvae import VAE\n",
    "\n",
    "vae = VAE()\n",
    "# Load the state_dict into CPU memory\n",
    "state_dict = torch.load('../vae2.pth', map_location='cpu')\n",
    "\n",
    "# Remove 'module.' prefix from state_dict keys\n",
    "new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "# Load the modified state_dict into the model\n",
    "vae.load_state_dict(new_state_dict)\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    vae = vae.to(device)\n",
    "    print(\"Using GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the states (images) from a random episode's data\n",
    "episode_idx = np.random.randint(len(data))\n",
    "print(f\"Episode {episode_idx} is selected\")\n",
    "episode_data = data[episode_idx]\n",
    "states = [x[0] for x in episode_data]\n",
    "\n",
    "# Randomly sample 30 images\n",
    "#random_indices = np.random.choice(len(states), size=30, replace=False)\n",
    "# take 30 sequential images\n",
    "start = np.random.randint(len(states)-30)\n",
    "random_indices = np.arange(start, start+30)\n",
    "# take 30 sequential images skip 5 between starting at start\n",
    "random_indices = np.arange(start, start+30*5, 5)\n",
    "random_samples = [states[i] for i in random_indices]\n",
    "random_samples = np.array(random_samples)\n",
    "# Ensure random_samples has shape [batch_size, height, width]\n",
    "# Add channel dimension\n",
    "random_samples = np.expand_dims(random_samples, axis=1)\n",
    "\n",
    "# Convert to PyTorch tensor and normalize to range [0, 1]\n",
    "random_samples_tensor = torch.tensor(random_samples, dtype=torch.float32)/255.\n",
    "\n",
    "# Move tensor to the appropriate device\n",
    "random_samples_tensor = random_samples_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, mu, logvar = vae(random_samples_tensor)\n",
    "\n",
    "# Convert tensors back to numpy for plotting\n",
    "reconstructed = reconstructed.cpu().numpy()\n",
    "mu = mu.cpu().numpy()\n",
    "logvar = logvar.cpu().numpy()\n",
    "\n",
    "# Calculate reconstruction loss (MSE) for each image\n",
    "reconstruction_loss = np.sum((random_samples/255. - reconstructed)**2, axis=(1, 2, 3))\n",
    "# Calculate KL divergence loss for each image\n",
    "kl_loss = -0.5 * np.sum(1 + logvar - np.square(mu) - np.exp(logvar), axis=1)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Plotting original images\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Original Images')\n",
    "grid_img = make_grid(torch.tensor(random_samples), nrow=5)\n",
    "plt.imshow(np.transpose(grid_img, (1, 2, 0)))\n",
    "\n",
    "# Plotting reconstructed images\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Reconstructed Images')\n",
    "grid_img_reconstructed = make_grid(torch.tensor(reconstructed), nrow=5)\n",
    "plt.imshow(np.transpose(grid_img_reconstructed, (1, 2, 0)))\n",
    "\n",
    "# Calculate position for annotations\n",
    "img_size = 64  # Assuming each image is 64x64\n",
    "grid_size = 5  # 5 images in a row\n",
    "padding = 2  # Default padding in make_grid\n",
    "\n",
    "# Annotate with Reconstruction loss and KL loss\n",
    "for i, (rec_loss, kl) in enumerate(zip(reconstruction_loss, kl_loss)):\n",
    "    row = i // grid_size\n",
    "    col = i % grid_size\n",
    "    x_text = col * (img_size + padding)\n",
    "    y_text = row * (img_size + padding)\n",
    "    plt.text(x_text + 5, y_text + 5, f\"MSE: {rec_loss:.2f}\", color='white', fontsize=8, ha='left', va='top')\n",
    "    plt.text(x_text + 5, y_text + 20, f\"KL: {kl:.2f}\", color='red', fontsize=8, ha='left', va='top')\n",
    "\n",
    "# Plotting latent vectors\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Latent Vectors')\n",
    "latent = mu + np.exp(0.5 * logvar) * np.random.randn(*logvar.shape)\n",
    "scatter = plt.scatter(latent[:, 0], latent[:, 1])\n",
    "\n",
    "# Annotate each point\n",
    "for i, (x, y) in enumerate(latent[:, :2]):\n",
    "    plt.annotate(str(i), (x, y), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# print the mean of the losses\n",
    "print(f\"Mean Reconstruction loss: {np.mean(reconstruction_loss):.2f}\")\n",
    "print(f\"Mean KL loss: {np.mean(kl_loss):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 10 random episodes\n",
    "num_random_episodes = 10\n",
    "random_episodes_indices = np.random.choice(len(data), num_random_episodes, replace=False)\n",
    "\n",
    "# Initialize an empty list to hold states from multiple episodes\n",
    "all_samples = []\n",
    "\n",
    "# Loop through the selected random episodes to collect states\n",
    "for episode_idx in random_episodes_indices:\n",
    "    episode_data = data[episode_idx]\n",
    "    states = [x[0] for x in episode_data]\n",
    "    all_samples.extend(states)\n",
    "\n",
    "# Convert the list to a NumPy array (assuming each state is a NumPy array)\n",
    "all_samples = np.array(all_samples)\n",
    "# Add channel dimension\n",
    "all_samples = np.expand_dims(all_samples, axis=1)\n",
    "\n",
    "# The shape of all_samples will be (num_samples, state_shape)\n",
    "print(f\"Collected a total of {len(all_samples)} states from {num_random_episodes} random episodes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensor and normalize to range [0, 1]\n",
    "all_samples_tensor = torch.tensor(all_samples, dtype=torch.float32)/255.\n",
    "\n",
    "# Move tensor to the appropriate device\n",
    "all_samples_tensor = all_samples_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, mu, logvar = vae(all_samples_tensor)\n",
    "\n",
    "mu = mu.cpu().numpy()\n",
    "logvar = logvar.cpu().numpy()\n",
    "latent = mu + np.exp(0.5 * logvar) * np.random.randn(*logvar.shape)\n",
    "\n",
    "# Assuming `all_labels` are the corresponding labels for `all_samples`\n",
    "plt.scatter(latent[:, 0], latent[:, 1])#, c=all_labels, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.title('Colored Latent Space 1st 2 dims')\n",
    "plt.xlabel('Latent Dim 1')\n",
    "plt.ylabel('Latent Dim 2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.kdeplot(x=latent[:, 0], y=latent[:, 1], cmap=\"Blues\", fill=True, thresh=0.05)\n",
    "plt.title('Density Plot of Latent Space only 1st 2 dims')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and std along each dimension of the latent space\n",
    "means = np.mean(latent, axis=0)\n",
    "std_devs = np.std(latent, axis=0)\n",
    "\n",
    "# Pick the two dimensions with the largest std_devs for visualization\n",
    "largest_std_dims = np.argsort(std_devs)[-2:]  # Get indices of largest std devs\n",
    "\n",
    "# Extract the values along the selected dimensions\n",
    "selected_latent = latent[:, largest_std_dims]\n",
    "\n",
    "# Create a large figure for the density plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Plot the density\n",
    "sns.kdeplot(x=selected_latent[:, 0], y=selected_latent[:, 1], cmap=\"Blues\", fill=True, thresh=0.05)\n",
    "\n",
    "# Calculate the number of images to overlay (1% in this example)\n",
    "num_to_overlay = int(len(all_samples) * 0.01)\n",
    "random_indices = np.random.choice(len(all_samples), num_to_overlay, replace=False)\n",
    "\n",
    "# Choose random images and their corresponding latent coordinates\n",
    "random_images = all_samples[random_indices]\n",
    "random_latent_coords = selected_latent[random_indices]\n",
    "\n",
    "# Overlay the randomly selected images\n",
    "scale_factor = 0.05  # Adjust as needed\n",
    "offset_x, offset_y = -1, -1  # Adjust based on your latent space\n",
    "\n",
    "for i, img in enumerate(random_images):\n",
    "    x, y = random_latent_coords[i]\n",
    "    x += offset_x\n",
    "    y += offset_y\n",
    "    plt.imshow(img[0], extent=[x, x + scale_factor, y, y + scale_factor], cmap='gray')  # Assuming grayscale images\n",
    "\n",
    "plt.title('Density Plot of Latent Space with Overlaid Images')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32  # Replace with the actual dimension\n",
    "image_height = 64  # Replace with the actual image height\n",
    "image_width = 64  # Replace with the actual image width\n",
    "\n",
    "latent_action_pairs = np.load('./../data/latent_action_pairs (6).npy')\n",
    "latent_vectors = latent_action_pairs[:, :latent_dim] \n",
    "actions = latent_action_pairs[:, latent_dim:]\n",
    "\n",
    "\n",
    "print(latent_action_pairs.shape)\n",
    "print(latent_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_action_pairs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one latent vector\n",
    "latent_vector = torch.tensor(latent_vectors[0], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "# Decode the latent vector\n",
    "with torch.no_grad():\n",
    "    decoded_img = vae.decode(latent_vector).squeeze().cpu().numpy()\n",
    "\n",
    "# If your decoded image is not in the shape you expect, reshape it accordingly\n",
    "decoded_img = decoded_img.reshape((image_height, image_width))  # Assuming grayscale images\n",
    "\n",
    "plt.imshow(decoded_img, cmap='gray')  # or cmap='color' if your image is color\n",
    "plt.title('Decoded Image from Latent Vector')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def decode_and_plot(idx):\n",
    "    latent_vector = torch.tensor(latent_vectors[idx], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Decode the latent vector\n",
    "    with torch.no_grad():\n",
    "        decoded_img = vae.decode(latent_vector).squeeze().cpu().numpy()\n",
    "    \n",
    "    decoded_img = decoded_img.reshape((image_height, image_width))\n",
    "    \n",
    "    # Create a subplot for original and decoded images\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Plot the original image\n",
    "    axes[0].imshow(sample_states[idx], cmap='gray')\n",
    "    axes[0].set_title(f'Original Image (Index {idx})')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Plot the decoded image\n",
    "    axes[1].imshow(decoded_img, cmap='gray')\n",
    "    axes[1].set_title(f'Decoded Image (Index {idx})')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a slider to select the latent vector index\n",
    "interact(decode_and_plot, idx=(0, len(sample_states) - 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATENTS ARE SHUFFLED NEED TO FIX - This is fixed there was not shuffle the first VAE model was bad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 10x10 grid for plotting\n",
    "fig, axes = plt.subplots(15, 15, figsize=(15, 15))\n",
    "\n",
    "# Flatten the 10x10 grid into a list for easy iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes_flat):\n",
    "    #Plot original image in original position\n",
    "    \n",
    "    \n",
    "    ax.imshow(sample_states[i], cmap='gray')\n",
    "    ax.set_title(f'Img {i+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 10x10 grid for plotting\n",
    "fig, axes = plt.subplots(15, 15, figsize=(15, 15))\n",
    "\n",
    "# Flatten the 10x10 grid into a list for easy iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes_flat):\n",
    "    latent_vector = torch.tensor(latent_vectors[i], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Decode the latent vector\n",
    "    with torch.no_grad():\n",
    "        decoded_img = vae.decode(latent_vector).squeeze().cpu().numpy()\n",
    "\n",
    "    # If your decoded image is not in the shape you expect, reshape it\n",
    "    decoded_img = decoded_img.reshape((image_height, image_width))  # Replace with your actual image dimensions\n",
    "    \n",
    "    ax.imshow(decoded_img, cmap='gray')  # or cmap='color' if your image is color\n",
    "    ax.set_title(f'Img {i+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Memory Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from worldmodels1.src.worldmodels1.mdnrnn import MemoryModel\n",
    "\n",
    "latent_dim = 32 \n",
    "action_dim = 3\n",
    "\n",
    "model = MemoryModel(n_input=latent_dim+action_dim, n_hidden=256, n_gaussians=5, latent_dim=latent_dim)\n",
    "# Load the state_dict into CPU memory\n",
    "state_dict = torch.load('./../src/worldmodels1/memory_model.pth', map_location='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy input of zeros\n",
    "dummy_input = torch.zeros(1, 1, latent_dim+action_dim)\n",
    "#dummy_input = torch.randn(1, 1, latent_dim+action_dim)\n",
    "print(dummy_input.shape)\n",
    "\n",
    "# make a forward pass\n",
    "pi, mu, sigma, _ = model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.size(), mu.size(), sigma.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_sample(pi, mu, sigma):\n",
    "    # Sample one Gaussian index based on pi values\n",
    "    gaussian_idx = torch.multinomial(pi[0, 0, :], 1)\n",
    "\n",
    "    # Pick the mu and sigma corresponding to the chosen Gaussian\n",
    "    chosen_mu = mu[0, 0, gaussian_idx, :]\n",
    "    chosen_sigma = sigma[0, 0, gaussian_idx, :]\n",
    "\n",
    "    # Sample a point from the chosen Gaussian distribution\n",
    "    sample = torch.normal(chosen_mu, chosen_sigma)\n",
    "    return sample\n",
    "\n",
    "# Assume pi, mu, sigma have been computed and have the following shapes\n",
    "# pi: (batch_size, n_gaussians)\n",
    "# mu: (batch_size, n_gaussians, latent_dim)\n",
    "# sigma: (batch_size, n_gaussians, latent_dim)\n",
    "vae.to('cpu')\n",
    "latent_output = gmm_sample(pi, mu, sigma)\n",
    "print(latent_output.shape)\n",
    "print(latent_output)\n",
    "\n",
    "# decode the latent vector and plot the decoded image\n",
    "with torch.no_grad():\n",
    "    decoded_img = vae.decode(latent_output.unsqueeze(0)).squeeze().cpu().numpy()\n",
    "\n",
    "plt.imshow(decoded_img, cmap='gray')  # or cmap='color' if your image is color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_calculate(pi, mu, sigma):\n",
    "    with torch.no_grad():\n",
    "        gaussians = torch.distributions.Normal(mu, sigma)\n",
    "        gaussians = gaussians.sample().squeeze()  # This might still be 2D depending on the original dimensions of mu and sigma\n",
    "        pi = pi.squeeze()\n",
    "\n",
    "        # Ensure dimensions are compatible for weighted sum\n",
    "        pi = pi.view(-1, 1)\n",
    "\n",
    "        #print(gaussians.shape)\n",
    "        #print(pi.shape)\n",
    "\n",
    "        # Use matmul for the weighted sum\n",
    "        predicted_latent = torch.matmul(pi.T, gaussians)\n",
    "\n",
    "    return predicted_latent\n",
    "\n",
    "\n",
    "#test mdn_calculate\n",
    "predicted_latent = mdn_calculate(pi, mu, sigma)\n",
    "print(predicted_latent.shape)\n",
    "print(predicted_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "hidden = None\n",
    "action1 = widgets.FloatText(value=0.0, description='Action1:')\n",
    "action2 = widgets.FloatText(value=0.0, description='Action2:')\n",
    "action3 = widgets.FloatText(value=0.0, description='Action3:')\n",
    "button = widgets.Button(description=\"Predict Next Frame\")\n",
    "out = widgets.Output()\n",
    "\n",
    "vae.to('cpu')\n",
    "current_state = torch.zeros(1, 1, latent_dim + action_dim)\n",
    "def on_button_click(b):\n",
    "    action = torch.Tensor([[[float(action1.value), float(action2.value), float(action3.value)]]])\n",
    "    #print(action)\n",
    "    global current_state\n",
    "    global hidden\n",
    "    current_state[:, :, -action_dim:] = action\n",
    "    #print(current_state)\n",
    "    with torch.no_grad():\n",
    "        pi, mu, sigma, hidden = model(current_state, hidden)\n",
    "        latent_output = gmm_sample(pi, mu, sigma)\n",
    "        #latent_output = mdn_calculate(pi, mu, sigma)\n",
    "        #print(latent_output)\n",
    "        current_state = torch.cat((latent_output.unsqueeze(0), action), dim=-1)\n",
    "        # Assume vae is your VAE model\n",
    "        decoded_img = vae.decode(latent_output.unsqueeze(0)).squeeze().cpu().numpy()\n",
    "    with out:\n",
    "        # Clear the previous frame\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Display the new frame\n",
    "        plt.imshow(decoded_img, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "display(action1, action2, action3, button, out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryModel(\n",
       "  (lstm): LSTM(35, 256, batch_first=True)\n",
       "  (mdn): MDN(\n",
       "    (z_h): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=325, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from worldmodels1.src.worldmodels1.cnnvae import VAE\n",
    "from worldmodels1.src.worldmodels1.mdnrnn import MemoryModel\n",
    "import torch\n",
    "\n",
    "latent_dim = 32\n",
    "action_dim = 3\n",
    "\n",
    "vae = VAE()\n",
    "# Load the state_dict into CPU memory\n",
    "state_dict = torch.load('./../vae2.pth', map_location='cpu')\n",
    "# Remove 'module.' prefix from state_dict keys\n",
    "new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# Load the modified state_dict into the model\n",
    "vae.load_state_dict(new_state_dict)\n",
    "vae.eval()\n",
    "\n",
    "rnn = MemoryModel(n_input=latent_dim+action_dim, n_hidden=256, n_gaussians=5, latent_dim=latent_dim)\n",
    "# Load the state_dict into CPU memory\n",
    "state_dict = torch.load('./../src/worldmodels1/memory_model.pth', map_location='cpu')\n",
    "# Remove 'module.' prefix from state_dict keys\n",
    "new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# Load the modified state_dict into the model\n",
    "rnn.load_state_dict(new_state_dict)\n",
    "rnn.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_obs(obs, action, vae, rnn, hidden, device='cpu'):\n",
    "    # obs is 96x96x3 need to convert to 64x64x1. \n",
    "    # Convert to PyTorch tensor and normalize and permute dimensions and transfer to device\n",
    "    obs = torch.from_numpy(obs).permute(2, 0, 1).float().to(device) / 255.0  \n",
    "    # Rescale and average color channels\n",
    "    obs = torch.nn.functional.interpolate(obs.unsqueeze(0), size=(64, 64), mode='bilinear', align_corners=False)\n",
    "    obs = obs.mean(dim=1, keepdim=True)  # Reduce color channels by taking the mean\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #print(f'obs shape: {obs.shape}')\n",
    "        mu, logvar = vae.encode(obs)\n",
    "        z_t = vae.reparameterize(mu, logvar)\n",
    "        #print(f'z_t shape: {z_t.shape}')\n",
    "        # check if action is a tensor\n",
    "        if not torch.is_tensor(action):\n",
    "            action = torch.tensor(action).float().to(device).unsqueeze(0)\n",
    "        rnn_in = torch.cat((z_t, action), dim=1).unsqueeze(0)\n",
    "        #print(f\"obs shape: {obs.shape}\")\n",
    "        #print(f\"action shape: {action.shape}\")\n",
    "        #print(f\"rnn_in shape: {rnn_in.shape}\")\n",
    "        #print(f\"Initial hidden states shapes: {self.hidden[0].shape}, {self.hidden[1].shape}\")\n",
    "        _, _, _, hidden = rnn(rnn_in, hidden)\n",
    "        # extract hidden state\n",
    "        h_t = hidden[0]\n",
    "        #print(f\"Final hidden states shapes: {self.hidden[0].shape}, {self.hidden[1].shape}\")\n",
    "        #print(f\"h_t shape: {h_t.shape}\")\n",
    "        #print(f\"z_t shape: {z_t.shape}\")\n",
    "        # concat z_t and hidden\n",
    "        z_t = z_t.squeeze(0) # remove batch dimension\n",
    "        h_t = h_t.squeeze(0).squeeze(0) # remove batch and sequence dimensions\n",
    "        obs = torch.cat((z_t, h_t))\n",
    "        #print(f\"obs shape: {obs.shape}\")\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aswegen.d\\Dropbox\\0_Buas\\BuasDev\\World Models\\worldmodels1\\notebooks\\explore_data.ipynb Cell 39\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     show_state(env)  \u001b[39m# Display the environment\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     obs \u001b[39m=\u001b[39m process_obs(obs, action, vae, rnn, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# radnom obs for now of shape (32 +256)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m#obs = np.random.randn(32+256)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\aswegen.d\\Dropbox\\0_Buas\\BuasDev\\World Models\\worldmodels1\\notebooks\\explore_data.ipynb Cell 39\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m plt\u001b[39m.\u001b[39mclf()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(env\u001b[39m.\u001b[39;49mrender())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswegen.d/Dropbox/0_Buas/BuasDev/World%20Models/worldmodels1/notebooks/explore_data.ipynb#X46sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m display\u001b[39m.\u001b[39mdisplay(plt\u001b[39m.\u001b[39mgcf())\n",
      "File \u001b[1;32mc:\\Users\\aswegen.d\\Dropbox\\0_Buas\\BuasDev\\World Models\\worldmodels1\\.venv\\Lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RenderFrame \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[RenderFrame] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[1;32mc:\\Users\\aswegen.d\\Dropbox\\0_Buas\\BuasDev\\World Models\\worldmodels1\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aswegen.d\\Dropbox\\0_Buas\\BuasDev\\World Models\\worldmodels1\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m env_render_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aswegen.d\\Dropbox\\0_Buas\\BuasDev\\World Models\\worldmodels1\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:589\u001b[0m, in \u001b[0;36mCarRacing.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    587\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32mc:\\Users\\aswegen.d\\Dropbox\\0_Buas\\BuasDev\\World Models\\worldmodels1\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:617\u001b[0m, in \u001b[0;36mCarRacing._render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    614\u001b[0m trans \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mVector2((scroll_x, scroll_y))\u001b[39m.\u001b[39mrotate_rad(angle)\n\u001b[0;32m    615\u001b[0m trans \u001b[39m=\u001b[39m (WINDOW_W \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m trans[\u001b[39m0\u001b[39m], WINDOW_H \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m \u001b[39m+\u001b[39m trans[\u001b[39m1\u001b[39m])\n\u001b[1;32m--> 617\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_road(zoom, trans, angle)\n\u001b[0;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcar\u001b[39m.\u001b[39mdraw(\n\u001b[0;32m    619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf,\n\u001b[0;32m    620\u001b[0m     zoom,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m     mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mstate_pixels_list\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstate_pixels\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    624\u001b[0m )\n\u001b[0;32m    626\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mflip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, \u001b[39mFalse\u001b[39;00m, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\aswegen.d\\Dropbox\\0_Buas\\BuasDev\\World Models\\worldmodels1\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:661\u001b[0m, in \u001b[0;36mCarRacing._render_road\u001b[1;34m(self, zoom, translation, angle)\u001b[0m\n\u001b[0;32m    653\u001b[0m field \u001b[39m=\u001b[39m [\n\u001b[0;32m    654\u001b[0m     (bounds, bounds),\n\u001b[0;32m    655\u001b[0m     (bounds, \u001b[39m-\u001b[39mbounds),\n\u001b[0;32m    656\u001b[0m     (\u001b[39m-\u001b[39mbounds, \u001b[39m-\u001b[39mbounds),\n\u001b[0;32m    657\u001b[0m     (\u001b[39m-\u001b[39mbounds, bounds),\n\u001b[0;32m    658\u001b[0m ]\n\u001b[0;32m    660\u001b[0m \u001b[39m# draw background\u001b[39;00m\n\u001b[1;32m--> 661\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_draw_colored_polygon(\n\u001b[0;32m    662\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msurf, field, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbg_color, zoom, translation, angle, clip\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    663\u001b[0m )\n\u001b[0;32m    665\u001b[0m \u001b[39m# draw grass patches\u001b[39;00m\n\u001b[0;32m    666\u001b[0m grass \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\aswegen.d\\Dropbox\\0_Buas\\BuasDev\\World Models\\worldmodels1\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:775\u001b[0m, in \u001b[0;36mCarRacing._draw_colored_polygon\u001b[1;34m(self, surface, poly, color, zoom, translation, angle, clip)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m clip \u001b[39mor\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[0;32m    770\u001b[0m     (\u001b[39m-\u001b[39mMAX_SHAPE_DIM \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m coord[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m WINDOW_W \u001b[39m+\u001b[39m MAX_SHAPE_DIM)\n\u001b[0;32m    771\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39m-\u001b[39mMAX_SHAPE_DIM \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m coord[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m WINDOW_H \u001b[39m+\u001b[39m MAX_SHAPE_DIM)\n\u001b[0;32m    772\u001b[0m     \u001b[39mfor\u001b[39;00m coord \u001b[39min\u001b[39;00m poly\n\u001b[0;32m    773\u001b[0m ):\n\u001b[0;32m    774\u001b[0m     gfxdraw\u001b[39m.\u001b[39maapolygon(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, poly, color)\n\u001b[1;32m--> 775\u001b[0m     gfxdraw\u001b[39m.\u001b[39;49mfilled_polygon(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msurf, poly, color)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: plot the reconstructed image and the predicted next image\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym \n",
    "from worldmodels1.src.worldmodels1.gym_wrapper import CarRacingWrapper\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Load the trained model\n",
    "controller = PPO.load(\"./../../ppo_car_racing_56.zip\")\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CarRacing-v2', render_mode='rgb_array')\n",
    "\n",
    "# Function to display the environment\n",
    "def show_state(env):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "# Run the trained model\n",
    "for episode in range(1, 6):  # Run for 5 episodes\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    hidden = (torch.zeros((1, 1, rnn.n_hidden)).to(device), \n",
    "               torch.zeros((1, 1, rnn.n_hidden)).to(device))\n",
    "        # action should be a 1x3 tensor\n",
    "    action = torch.zeros((1, 3)).to(device)\n",
    "\n",
    "    \n",
    "    while not done:\n",
    "        show_state(env)  # Display the environment\n",
    "        obs = process_obs(obs, action, vae, rnn, hidden)\n",
    "        # radnom obs for now of shape (32 +256)\n",
    "        #obs = np.random.randn(32+256)\n",
    "        action, _ = controller.predict(obs)\n",
    "        # random action\n",
    "        # action = env.action_space.sample()\n",
    "        # action = action + np.array([0.0, 0.2, -0.45])\n",
    "        #action[2] = 0.0\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "    print(f\"Episode {episode} reward: {episode_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
